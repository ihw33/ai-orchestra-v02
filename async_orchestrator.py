#!/usr/bin/env python3
"""
ë¹„ë™ê¸° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - ì„±ëŠ¥ ìµœì í™” ë²„ì „
asyncioë¥¼ ì‚¬ìš©í•œ ì§„ì§œ ë¹„ë™ê¸° ì²˜ë¦¬
"""

import asyncio
import json
import time
from typing import Dict, List, Optional, Any
from datetime import datetime
import subprocess
from cache_manager import AIResponseCache, GitHubCache
from error_handler import ErrorHandler, log_info, log_error

class AsyncOrchestrator:
    """ë¹„ë™ê¸° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(self):
        self.ai_cache = AIResponseCache()
        self.gh_cache = GitHubCache()
        self.error_handler = ErrorHandler("AsyncOrchestrator")
        
        # AI ì„¤ì •
        self.ai_configs = {
            'gemini': {'command': 'gemini -p', 'timeout': 120},
            'claude': {'command': 'claude -p', 'timeout': 180},
            'codex': {'command': 'codex exec', 'timeout': 150}
        }
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'total_time': 0,
            'ai_calls': 0,
            'cache_hits': 0,
            'parallel_executions': 0
        }
    
    async def execute_ai_async(self, ai_name: str, prompt: str) -> Dict:
        """ë¹„ë™ê¸° AI ì‹¤í–‰"""
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸
        cached = self.ai_cache.get_cached_response(ai_name, prompt)
        if cached:
            self.stats['cache_hits'] += 1
            log_info(f"ğŸ’¾ Cache hit for {ai_name}")
            return {
                'success': True,
                'output': cached,
                'ai': ai_name,
                'cached': True,
                'time': 0
            }
        
        # AI ì‹¤í–‰
        self.stats['ai_calls'] += 1
        config = self.ai_configs[ai_name]
        cmd = f'{config["command"]} "{prompt}"'
        
        try:
            # subprocessë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            proc = await asyncio.create_subprocess_shell(
                cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            try:
                stdout, stderr = await asyncio.wait_for(
                    proc.communicate(),
                    timeout=config['timeout']
                )
            except asyncio.TimeoutError:
                proc.kill()
                await proc.communicate()
                log_error(f"â±ï¸ {ai_name} timeout")
                return {
                    'success': False,
                    'error': 'Timeout',
                    'ai': ai_name,
                    'time': time.time() - start_time
                }
            
            if proc.returncode == 0:
                output = stdout.decode()
                # ìºì‹œ ì €ì¥
                self.ai_cache.cache_response(ai_name, prompt, output)
                log_info(f"âœ… {ai_name} completed in {time.time() - start_time:.2f}s")
                return {
                    'success': True,
                    'output': output,
                    'ai': ai_name,
                    'cached': False,
                    'time': time.time() - start_time
                }
            else:
                log_error(f"âŒ {ai_name} failed")
                return {
                    'success': False,
                    'error': stderr.decode(),
                    'ai': ai_name,
                    'time': time.time() - start_time
                }
                
        except Exception as e:
            log_error(f"âŒ {ai_name} exception: {e}")
            return {
                'success': False,
                'error': str(e),
                'ai': ai_name,
                'time': time.time() - start_time
            }
    
    async def execute_parallel(self, ai_list: List[str], prompt: str) -> Dict[str, Dict]:
        """ë³‘ë ¬ AI ì‹¤í–‰ (ì§„ì§œ ë¹„ë™ê¸°)"""
        start_time = time.time()
        self.stats['parallel_executions'] += 1
        
        log_info(f"ğŸš€ Starting parallel execution for {len(ai_list)} AIs")
        
        # ëª¨ë“  AIë¥¼ ë™ì‹œì— ì‹¤í–‰
        tasks = [
            self.execute_ai_async(ai, prompt)
            for ai in ai_list if ai in self.ai_configs
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì •ë¦¬
        final_results = {}
        for ai, result in zip(ai_list, results):
            if isinstance(result, Exception):
                final_results[ai] = {
                    'success': False,
                    'error': str(result),
                    'ai': ai
                }
            else:
                final_results[ai] = result
        
        total_time = time.time() - start_time
        self.stats['total_time'] += total_time
        
        log_info(f"âš¡ Parallel execution completed in {total_time:.2f}s")
        return final_results
    
    async def execute_sequential(self, ai_list: List[str], prompt: str) -> Dict[str, Dict]:
        """ìˆœì°¨ AI ì‹¤í–‰ (ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬)"""
        start_time = time.time()
        results = {}
        context = ""
        
        for ai_name in ai_list:
            if ai_name in self.ai_configs:
                full_prompt = f"{context}\n\n{prompt}" if context else prompt
                result = await self.execute_ai_async(ai_name, full_prompt)
                results[ai_name] = result
                
                # ì„±ê³µí•œ ê²½ìš° ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                if result.get('success'):
                    context = f"Previous {ai_name} output:\n{result['output'][:500]}\n"
        
        total_time = time.time() - start_time
        log_info(f"ğŸ“ Sequential execution completed in {total_time:.2f}s")
        return results
    
    async def process_github_issue_async(self, issue_number: int, repo: str = "ihw33/ai-orchestra-v02"):
        """GitHub ì´ìŠˆ ë¹„ë™ê¸° ì²˜ë¦¬"""
        log_info(f"ğŸ“‹ Processing issue #{issue_number}")
        
        # ìºì‹œëœ ì´ìŠˆ ì •ë³´ ì‚¬ìš©
        issue = self.gh_cache.get_issue(issue_number, repo)
        if not issue:
            log_error(f"Issue #{issue_number} not found")
            return None
        
        prompt = f"Issue: {issue.get('title', '')}\n\n{issue.get('body', '')}"
        
        # ëª¨ë“  AIì—ê²Œ ë³‘ë ¬ë¡œ ìš”ì²­
        results = await self.execute_parallel(['gemini', 'claude', 'codex'], prompt)
        
        # ê²°ê³¼ í¬ìŠ¤íŒ… (ì´ ë¶€ë¶„ë„ ë¹„ë™ê¸°ë¡œ ê°œì„  ê°€ëŠ¥)
        await self.post_results_async(issue_number, repo, results)
        
        return results
    
    async def post_results_async(self, issue_number: int, repo: str, results: Dict):
        """ê²°ê³¼ë¥¼ GitHubì— ë¹„ë™ê¸°ë¡œ í¬ìŠ¤íŒ…"""
        comment = "## ğŸš€ Async Orchestra Results\n\n"
        
        for ai_name, result in results.items():
            status = "âœ…" if result.get('success') else "âŒ"
            cached = "ğŸ’¾" if result.get('cached') else "ğŸ”„"
            time_taken = result.get('time', 0)
            
            comment += f"### {ai_name.upper()} {status} {cached}\n"
            comment += f"*Time: {time_taken:.2f}s*\n\n"
            
            if result.get('success'):
                output = result['output'][:500]
                comment += f"```\n{output}\n...\n```\n\n"
            else:
                comment += f"Error: {result.get('error', 'Unknown')}\n\n"
        
        # í†µê³„ ì¶”ê°€
        comment += f"\n---\n"
        comment += f"*Total AI calls: {self.stats['ai_calls']} | "
        comment += f"Cache hits: {self.stats['cache_hits']}*"
        
        # GitHubì— í¬ìŠ¤íŒ… (ì—¬ì „íˆ ë™ê¸°ì‹ì´ì§€ë§Œ ê°œì„  ê°€ëŠ¥)
        escaped = comment.replace('"', '\\"').replace('\n', '\\n')
        cmd = f'gh issue comment {issue_number} -R {repo} -b "{escaped}"'
        
        proc = await asyncio.create_subprocess_shell(cmd)
        await proc.communicate()
        log_info(f"âœ… Results posted to issue #{issue_number}")
    
    def get_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        avg_time = self.stats['total_time'] / max(self.stats['parallel_executions'], 1)
        cache_rate = (self.stats['cache_hits'] / max(self.stats['ai_calls'], 1)) * 100
        
        return {
            **self.stats,
            'avg_parallel_time': f"{avg_time:.2f}s",
            'cache_hit_rate': f"{cache_rate:.1f}%"
        }

async def benchmark_comparison():
    """ë™ê¸° vs ë¹„ë™ê¸° ì„±ëŠ¥ ë¹„êµ"""
    orchestrator = AsyncOrchestrator()
    prompt = "What is the best programming language?"
    
    print("ğŸƒ Performance Benchmark\n")
    
    # ë³‘ë ¬ ì‹¤í–‰ (ë¹„ë™ê¸°)
    print("âš¡ Async Parallel Execution:")
    start = time.time()
    results = await orchestrator.execute_parallel(['gemini', 'claude', 'codex'], prompt)
    async_time = time.time() - start
    print(f"Time: {async_time:.2f}s\n")
    
    # ìˆœì°¨ ì‹¤í–‰ (ë¹„êµìš©)
    print("ğŸŒ Sequential Execution:")
    start = time.time()
    results = await orchestrator.execute_sequential(['gemini', 'claude', 'codex'], prompt)
    seq_time = time.time() - start
    print(f"Time: {seq_time:.2f}s\n")
    
    # ê²°ê³¼
    print(f"ğŸ¯ Performance Improvement: {seq_time/async_time:.1f}x faster!")
    print(f"\nğŸ“Š Stats:")
    print(json.dumps(orchestrator.get_stats(), indent=2))

# ë©”ì¸ ì‹¤í–‰
async def main():
    """ë©”ì¸ ë¹„ë™ê¸° í•¨ìˆ˜"""
    orchestrator = AsyncOrchestrator()
    
    # í…ŒìŠ¤íŠ¸: ë³‘ë ¬ ì‹¤í–‰
    print("Testing async orchestrator...")
    results = await orchestrator.execute_parallel(
        ['gemini', 'claude', 'codex'],
        "Write a hello world in Python"
    )
    
    for ai, result in results.items():
        print(f"\n{ai}: {result.get('success', False)}")
        if result.get('success'):
            print(f"Output: {result['output'][:100]}...")
    
    print(f"\nğŸ“Š Performance Stats:")
    print(json.dumps(orchestrator.get_stats(), indent=2))

if __name__ == "__main__":
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    # asyncio.run(benchmark_comparison())
    
    # ì¼ë°˜ í…ŒìŠ¤íŠ¸
    asyncio.run(main())