#!/usr/bin/env python3
"""
í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - ëª¨ë“  ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ê¸°ëŠ¥ì„ í•˜ë‚˜ë¡œ
multi_ai_orchestrator + master_orchestrator + ê°œì„ ì‚¬í•­
"""

import os
import sys
import subprocess
import json
import time
import re
from typing import Dict, List, Optional, Any
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# ì—ëŸ¬ ì²˜ë¦¬ ëª¨ë“ˆ ì„í¬íŠ¸
from error_handler import (
    ErrorHandler, 
    retry_on_error, 
    handle_errors,
    SafeExecutor,
    AIExecutionError,
    GitHubAPIError,
    log_info, 
    log_warning, 
    log_error
)

# í˜ë¥´ì†Œë‚˜ ì‹œìŠ¤í…œ ì„í¬íŠ¸
try:
    from personas.auto_persona_injector import IssueWorkflowIntegration
    PERSONA_ENABLED = True
except ImportError:
    PERSONA_ENABLED = False
    log_warning("í˜ë¥´ì†Œë‚˜ ì‹œìŠ¤í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")

class UnifiedOrchestrator:
    """í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - ëª¨ë“  ê¸°ëŠ¥ì„ í•˜ë‚˜ë¡œ"""
    
    def __init__(self):
        # ê¸°ë³¸ ì„¤ì •
        self.repo = os.getenv('GITHUB_REPO', 'ihw33/ai-orchestra-v02')
        self.timeout = int(os.getenv('AI_TIMEOUT', '120'))
        
        # í˜ë¥´ì†Œë‚˜ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if PERSONA_ENABLED:
            self.persona_integration = IssueWorkflowIntegration()
            log_info("âœ¨ í˜ë¥´ì†Œë‚˜ ì‹œìŠ¤í…œ í™œì„±í™”ë¨")
        
        # AI ì„¤ì • (multi_ai_orchestratorì—ì„œ)
        self.ais = {
            'gemini': {
                'command': 'gemini -p',
                'role': 'Architect & Analyzer',
                'timeout': 120
            },
            'claude': {
                'command': 'claude -p',
                'role': 'Implementation & Code Review',
                'timeout': 180
            },
            'codex': {
                'command': 'codex exec',
                'role': 'Backend & API Development',
                'timeout': 150
            }
        }
        
        # íŒ¨í„´ ë§¤ì¹­ ê·œì¹™ (master_orchestratorì—ì„œ)
        self.pattern_rules = {
            r'ë¶„ì„|ê²€í† |í‰ê°€|íƒ€ë‹¹ì„±|ì¡°ì‚¬|ë¦¬ì„œì¹˜': 'ANALYSIS_PIPELINE',
            r'êµ¬í˜„|ê°œë°œ|ë§Œë“¤|ìƒì„±|ì½”ë”©|í”„ë¡œê·¸ë˜ë°': 'IMPLEMENTATION_PIPELINE',
            r'ë²„ê·¸|ì˜¤ë¥˜|ì—ëŸ¬|ìˆ˜ì •|ê³ ì¹˜|ë¬¸ì œ|ë””ë²„ê·¸': 'BUGFIX_WORKFLOW',
            r'í…ŒìŠ¤íŠ¸|ê²€ì¦|í™•ì¸|ì²´í¬': 'TEST_PIPELINE',
            r'ë¬¸ì„œ|ë„í|ì„¤ëª…|ê°€ì´ë“œ': 'DOCUMENTATION_PIPELINE',
            r'ìµœì í™”|ê°œì„ |ì„±ëŠ¥|ì†ë„': 'OPTIMIZATION_PIPELINE'
        }
        
        # ì›Œí¬í”Œë¡œìš° ì •ì˜
        self.workflows = {
            'ANALYSIS_PIPELINE': ['gemini', 'claude'],
            'IMPLEMENTATION_PIPELINE': ['gemini', 'codex', 'claude'],
            'BUGFIX_WORKFLOW': ['claude', 'codex'],
            'TEST_PIPELINE': ['codex', 'gemini'],
            'DOCUMENTATION_PIPELINE': ['gemini', 'claude'],
            'OPTIMIZATION_PIPELINE': ['codex', 'claude', 'gemini']
        }
        
        # ì‹¤í–‰ í†µê³„
        self.stats = {
            'total_requests': 0,
            'successful': 0,
            'failed': 0,
            'ai_calls': {}
        }
        
        # íˆìŠ¤í† ë¦¬
        self.history = []
        self.load_history()
        
        # ìŠ¤ë ˆë“œ í’€ (ë³‘ë ¬ ì‹¤í–‰ìš©)
        self.executor = ThreadPoolExecutor(max_workers=3)
    
    # ==================== GitHub ì´ìŠˆ ì²˜ë¦¬ ====================
    
    def process_github_issue(self, issue_number: int, parallel: bool = True) -> Dict:
        """GitHub ì´ìŠˆ ì²˜ë¦¬ - ë³‘ë ¬ ë˜ëŠ” ìˆœì°¨"""
        print(f"\nğŸ” Processing Issue #{issue_number}")
        
        # ì´ìŠˆ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        issue_content = self._get_issue_content(issue_number)
        if not issue_content:
            return {'success': False, 'error': 'Issue not found'}
        
        # í˜ë¥´ì†Œë‚˜ ëª¨ë“œ í™•ì¸ ([AI] íƒœê·¸ê°€ ìˆê³  í˜ë¥´ì†Œë‚˜ ì‹œìŠ¤í…œì´ í™œì„±í™”ëœ ê²½ìš°)
        use_personas = False
        if PERSONA_ENABLED and '[AI]' in issue_content.get('title', ''):
            use_personas = True
            log_info("ğŸ­ í˜ë¥´ì†Œë‚˜ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤")
        
        if use_personas:
            # í˜ë¥´ì†Œë‚˜ ì‹œìŠ¤í…œìœ¼ë¡œ ì²˜ë¦¬
            try:
                self.persona_integration.on_issue_created(issue_number)
                return {
                    'success': True,
                    'issue': issue_number,
                    'mode': 'persona',
                    'message': 'í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ AI íŒ€ì´ ì‘ì—…ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤'
                }
            except Exception as e:
                log_error(f"í˜ë¥´ì†Œë‚˜ ëª¨ë“œ ì‹¤íŒ¨: {e}")
                log_info("ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±í•©ë‹ˆë‹¤")
                use_personas = False
        
        # ê¸°ë³¸ ëª¨ë“œ (í˜ë¥´ì†Œë‚˜ ë¯¸ì‚¬ìš© ë˜ëŠ” ì‹¤íŒ¨ ì‹œ)
        if not use_personas:
            # íŒ¨í„´ ê°ì§€
            pattern = self.detect_pattern(issue_content['title'] + ' ' + issue_content.get('body', ''))
            workflow = self.workflows.get(pattern, ['gemini', 'claude', 'codex'])
            
            print(f"ğŸ“‹ Pattern: {pattern}")
            print(f"ğŸ¤– Workflow: {' â†’ '.join(workflow)}")
            
            # AI ì‹¤í–‰
            if parallel:
                results = self._execute_parallel(workflow, issue_content)
            else:
                results = self._execute_sequential(workflow, issue_content)
        
        # ê²°ê³¼ GitHubì— í¬ìŠ¤íŒ…
        self._post_results_to_issue(issue_number, results)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats['total_requests'] += 1
        if all(r.get('success') for r in results.values()):
            self.stats['successful'] += 1
        else:
            self.stats['failed'] += 1
        
        return {
            'success': True,
            'issue': issue_number,
            'pattern': pattern,
            'results': results
        }
    
    def _get_issue_content(self, issue_number: int) -> Optional[Dict]:
        """GitHub ì´ìŠˆ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            cmd = f"gh issue view {issue_number} -R {self.repo} --json title,body,labels"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                return json.loads(result.stdout)
        except Exception as e:
            print(f"âŒ Error fetching issue: {e}")
        return None
    
    def _execute_parallel(self, workflow: List[str], issue_content: Dict) -> Dict:
        """ë³‘ë ¬ AI ì‹¤í–‰"""
        results = {}
        futures = {}
        
        with ThreadPoolExecutor(max_workers=len(workflow)) as executor:
            # ëª¨ë“  AI ë™ì‹œ ì‹¤í–‰
            for ai_name in workflow:
                if ai_name in self.ais:
                    prompt = self._create_prompt(ai_name, issue_content)
                    future = executor.submit(self._execute_ai, ai_name, prompt)
                    futures[future] = ai_name
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(futures):
                ai_name = futures[future]
                try:
                    results[ai_name] = future.result(timeout=self.timeout)
                except Exception as e:
                    results[ai_name] = {'success': False, 'error': str(e)}
        
        return results
    
    def _execute_sequential(self, workflow: List[str], issue_content: Dict) -> Dict:
        """ìˆœì°¨ AI ì‹¤í–‰ (ì´ì „ ê²°ê³¼ ì°¸ì¡°)"""
        results = {}
        context = ""
        
        for ai_name in workflow:
            if ai_name in self.ais:
                # ì´ì „ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€
                prompt = self._create_prompt(ai_name, issue_content, context)
                result = self._execute_ai(ai_name, prompt)
                results[ai_name] = result
                
                # ì„±ê³µí•œ ê²½ìš° ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                if result.get('success'):
                    context = f"Previous {ai_name} analysis:\\n{result['output'][:500]}\\n\\n"
        
        return results
    
    def _execute_ai(self, ai_name: str, prompt: str) -> Dict:
        """ë‹¨ì¼ AI ì‹¤í–‰"""
        print(f"  ğŸ¤– Executing {ai_name}...")
        
        ai_config = self.ais[ai_name]
        cmd = f'{ai_config["command"]} "{prompt}"'
        
        try:
            result = subprocess.run(
                cmd,
                shell=True,
                capture_output=True,
                text=True,
                timeout=ai_config.get('timeout', self.timeout)
            )
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            if ai_name not in self.stats['ai_calls']:
                self.stats['ai_calls'][ai_name] = 0
            self.stats['ai_calls'][ai_name] += 1
            
            if result.returncode == 0:
                print(f"  âœ… {ai_name} completed")
                return {
                    'success': True,
                    'output': result.stdout,
                    'timestamp': datetime.now().isoformat()
                }
            else:
                print(f"  âŒ {ai_name} failed")
                return {
                    'success': False,
                    'error': result.stderr,
                    'timestamp': datetime.now().isoformat()
                }
                
        except subprocess.TimeoutExpired:
            print(f"  â±ï¸ {ai_name} timeout")
            return {'success': False, 'error': 'Timeout'}
        except Exception as e:
            print(f"  âŒ {ai_name} error: {e}")
            return {'success': False, 'error': str(e)}
    
    def _create_prompt(self, ai_name: str, issue_content: Dict, context: str = "") -> str:
        """AIë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        role = self.ais[ai_name]['role']
        title = issue_content.get('title', '')
        body = issue_content.get('body', '')
        
        prompt = f"""Role: {role}
Task: {title}

{context}

Details:
{body}

Please provide your analysis and recommendations."""
        
        return prompt
    
    def _post_results_to_issue(self, issue_number: int, results: Dict):
        """ê²°ê³¼ë¥¼ GitHub ì´ìŠˆì— í¬ìŠ¤íŒ…"""
        comment = "## ğŸ¤– AI Orchestra Results\\n\\n"
        
        for ai_name, result in results.items():
            status = "âœ…" if result.get('success') else "âŒ"
            comment += f"### {ai_name.upper()} {status}\\n"
            
            if result.get('success'):
                output = result['output'][:1000]
                comment += f"```\\n{output}\\n```\\n\\n"
            else:
                comment += f"Error: {result.get('error', 'Unknown')}\\n\\n"
        
        # í†µê³„ ì¶”ê°€
        comment += f"\\n---\\n"
        comment += f"*Executed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\\n"
        comment += f"*Total AI calls: {sum(self.stats['ai_calls'].values())}*"
        
        # GitHubì— ì½”ë©˜íŠ¸
        try:
            escaped = comment.replace('"', '\\"').replace('\n', '\\n')
            cmd = f'gh issue comment {issue_number} -R {self.repo} -b "{escaped}"'
            subprocess.run(cmd, shell=True, capture_output=True, text=True)
            print(f"âœ… Results posted to issue #{issue_number}")
        except Exception as e:
            print(f"âŒ Failed to post results: {e}")
    
    # ==================== íŒ¨í„´ ë§¤ì¹­ & ì›Œí¬í”Œë¡œìš° ====================
    
    def detect_pattern(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ê°ì§€"""
        text_lower = text.lower()
        
        # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë§¤ì¹­
        for regex, pattern in self.pattern_rules.items():
            if re.search(regex, text_lower):
                return pattern
        
        return 'GENERAL_PIPELINE'
    
    def process_request(self, request: str, mode: str = 'auto') -> Dict:
        """ì¼ë°˜ ìš”ì²­ ì²˜ë¦¬ (GitHub ì´ìŠˆ ì—†ì´)"""
        print(f"\nğŸ“ Processing: {request}")
        
        # íŒ¨í„´ ê°ì§€
        pattern = self.detect_pattern(request)
        workflow = self.workflows.get(pattern, ['gemini', 'claude'])
        
        print(f"ğŸ¯ Pattern: {pattern}")
        print(f"ğŸ¤– Workflow: {' â†’ '.join(workflow)}")
        
        # ê°€ìƒ ì´ìŠˆ ë‚´ìš© ìƒì„±
        issue_content = {
            'title': request,
            'body': ''
        }
        
        # ì‹¤í–‰ ëª¨ë“œ ê²°ì •
        if mode == 'parallel':
            results = self._execute_parallel(workflow, issue_content)
        elif mode == 'sequential':
            results = self._execute_sequential(workflow, issue_content)
        else:
            # ìë™ ê²°ì •: 3ê°œ ì´ìƒì´ë©´ ë³‘ë ¬, ì•„ë‹ˆë©´ ìˆœì°¨
            if len(workflow) >= 3:
                results = self._execute_parallel(workflow, issue_content)
            else:
                results = self._execute_sequential(workflow, issue_content)
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.history.append({
            'request': request,
            'pattern': pattern,
            'workflow': workflow,
            'results': results,
            'timestamp': datetime.now().isoformat()
        })
        self.save_history()
        
        return {
            'success': True,
            'request': request,
            'pattern': pattern,
            'results': results
        }
    
    # ==================== ëŒ€í™”í˜• ëª¨ë“œ ====================
    
    def interactive_mode(self):
        """ëŒ€í™”í˜• ëª¨ë“œ"""
        print("=" * 60)
        print("ğŸ¤– Unified Orchestrator - Interactive Mode")
        print("=" * 60)
        print("\nCommands:")
        print("  /help     - Show help")
        print("  /stats    - Show statistics")
        print("  /history  - Show history")
        print("  /issue N  - Process GitHub issue #N")
        print("  /exit     - Exit")
        print("\nOr type any request to process it.")
        print("-" * 60)
        
        while True:
            try:
                request = input("\nğŸ¯ > ").strip()
                
                if not request:
                    continue
                
                if request == '/exit':
                    print("ğŸ‘‹ Goodbye!")
                    break
                elif request == '/help':
                    self.show_help()
                elif request == '/stats':
                    self.show_stats()
                elif request == '/history':
                    self.show_history()
                elif request.startswith('/issue '):
                    issue_num = int(request.split()[1])
                    self.process_github_issue(issue_num)
                else:
                    self.process_request(request)
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Interrupted")
                break
            except Exception as e:
                print(f"âŒ Error: {e}")
    
    def show_help(self):
        """ë„ì›€ë§ í‘œì‹œ"""
        print("""
ğŸ“š Unified Orchestrator Help

Patterns:
- Analysis: ë¶„ì„, ê²€í† , í‰ê°€
- Implementation: êµ¬í˜„, ê°œë°œ, ìƒì„±
- Bugfix: ë²„ê·¸, ì˜¤ë¥˜, ìˆ˜ì •
- Test: í…ŒìŠ¤íŠ¸, ê²€ì¦
- Documentation: ë¬¸ì„œ, ê°€ì´ë“œ
- Optimization: ìµœì í™”, ê°œì„ 

Usage:
  python3 unified_orchestrator.py                    # Interactive mode
  python3 unified_orchestrator.py --issue 63        # Process issue
  python3 unified_orchestrator.py "request"         # Direct request
  python3 unified_orchestrator.py --stats           # Show statistics
        """)
    
    def show_stats(self):
        """í†µê³„ í‘œì‹œ"""
        print("\nğŸ“Š Statistics")
        print("-" * 40)
        print(f"Total Requests: {self.stats['total_requests']}")
        print(f"Successful: {self.stats['successful']}")
        print(f"Failed: {self.stats['failed']}")
        print("\nAI Calls:")
        for ai, count in self.stats['ai_calls'].items():
            print(f"  {ai}: {count}")
    
    def show_history(self):
        """íˆìŠ¤í† ë¦¬ í‘œì‹œ"""
        print("\nğŸ“œ Recent History")
        print("-" * 40)
        for item in self.history[-5:]:
            print(f"[{item['timestamp'][:19]}] {item['request'][:50]}")
            print(f"  Pattern: {item['pattern']}")
            print(f"  Success: {all(r.get('success') for r in item['results'].values())}")
    
    def save_history(self):
        """íˆìŠ¤í† ë¦¬ ì €ì¥"""
        try:
            with open('.orchestrator_history.json', 'w') as f:
                json.dump(self.history[-100:], f)  # ìµœê·¼ 100ê°œë§Œ
        except:
            pass
    
    def load_history(self):
        """íˆìŠ¤í† ë¦¬ ë¡œë“œ"""
        try:
            if os.path.exists('.orchestrator_history.json'):
                with open('.orchestrator_history.json', 'r') as f:
                    self.history = json.load(f)
        except:
            self.history = []
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.executor.shutdown(wait=True)
        self.save_history()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Unified AI Orchestrator')
    parser.add_argument('request', nargs='?', help='Direct request')
    parser.add_argument('--issue', type=int, help='Process GitHub issue')
    parser.add_argument('--parallel', action='store_true', help='Force parallel execution')
    parser.add_argument('--sequential', action='store_true', help='Force sequential execution')
    parser.add_argument('--stats', action='store_true', help='Show statistics')
    parser.add_argument('--help-extended', action='store_true', help='Show extended help')
    
    args = parser.parse_args()
    
    orchestrator = UnifiedOrchestrator()
    
    try:
        if args.help_extended:
            orchestrator.show_help()
        elif args.stats:
            orchestrator.show_stats()
        elif args.issue:
            orchestrator.process_github_issue(args.issue, parallel=not args.sequential)
        elif args.request:
            mode = 'parallel' if args.parallel else 'sequential' if args.sequential else 'auto'
            orchestrator.process_request(args.request, mode)
        else:
            orchestrator.interactive_mode()
    finally:
        orchestrator.cleanup()

if __name__ == "__main__":
    main()